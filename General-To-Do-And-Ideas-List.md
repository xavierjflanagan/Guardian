Agentic AI assistant 
design and sprint planning session. 
    Need to design out what i actually want the app to look like, but after i set up the pipeline 
    DIFFERENTIATION is crucial, we dont want AI out of the box look. Needs to look different and standout and be complelety new (which is it is)
Neural relational database:
    A neural mapping protocol, that when executed, goes through your code and maps out everything in a relational neural network (2D or 3D viewing for the user, but not really built for user viewing) 
    The neural mapping protocol runs through every bit of code you have, documenting and recording their relationships/effects (incoming or outgoing relationships) with.. anything. This anything can be categorised into types. Types could be; internal code (other parts of the code base), external code, applications, virtual events. 
    The neural mapping protocol runs through the codebase several times, building out the relational network. 
    Relational links between two objects within the neural relational database can have varying degrees of intensity, with a ranking system score attached to that relational intensity (this parralels the use it or lose it theory of neural dev, where higher use results in stronger connection). 
    They neural relational mapping protocol may require 'testing agents' that run through the system like a real world user would. This run-through results in a trail of 'activated code' which is recorded by the mapping protocol. This would happen hundreds of times with varying user scenarios programmed and executed by the 'testing agents'. 
    This all results in a neural network relational database that allows you to select any 'cell body' (piece of data) within the network, and immediately see the connections (if any) that exist between that cell body and other cell bodies within the network, as well as the intensity of connection. 
    This whole system approach is kind of what im doing with my AI document processing pipeline... We use the AI to analyse each segment of code within the codebase (just like how AI processes each data point within a health document), the AI applies metadata to that segment of code (purpose, outgoing and incoming effects, what it touches, what it contributes to etc). After it has analyzed a segment of code or a file of code, the AI has a produced a swath of metadata attached to each segment of code. It assigns each section of code a uuid, as well as a uuid for the file that its in, as well as uuid for the folder that the file is in and so on and so on. If its a slack message or email, then it gets a slack or email specific uuid etc. So, after the AI has analysed and processed a segment of code, it has produced a whole data set of metadata that is all attached to a single uuid. That uuid + organized metadata in the form on jsonb is stored in a relational database, which can then be searched and organized and filtered and pulled in anyway you like. We can then apply knn tactics to it. 
    Use case: We want AI to edit a piece of code within a file. AI then grabs the uuid of that file, finds all the sub uuids for all the segments of code within that file uuid, then, through the relational database of metadata that has been built, the AI knows which other uuids (files or code segments, or slack/mail messages) that are actually relevant to the file that the AI has been instructed to edit. The AI now has all relevant context. 

1. need to add components in data ingestion that confirm its relevant info that is a) worthy of AI analysis compute, and B) is medically relevant, and C) information is about the patient and not someone else.
    That it is a medical related document, and it does have health information. 
    Flag label data point if it cant be confirmed that it is about the patient (no identifying info on the page that contains the info)
    Flag label data point if it cant be confirmed as a authentic document (written by healthcare provider)
