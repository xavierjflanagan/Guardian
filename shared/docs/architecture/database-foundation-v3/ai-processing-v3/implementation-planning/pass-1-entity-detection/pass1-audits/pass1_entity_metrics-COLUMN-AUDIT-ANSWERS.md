# Pass 1 Entity Metrics Table Audit - Complete & Final

**Date:** 2025-10-08
**Status:** ✅ **AUDIT COMPLETE** - Token breakdown migration executed
**Context:** Comprehensive analysis of all 18 columns in `pass1_entity_metrics` table

**Final Verdict:**
- **Total Columns Reviewed:** 18 (100% coverage)
- **Columns to Keep:** 18 ✅
- **Columns to Remove:** 0 (2 already removed via Migration 15)
- **Architecture:** Session-level metrics with accurate token breakdown

**Migrations Executed:**
- ✅ Migration 15: Added `input_tokens`, `output_tokens`, `total_tokens`; removed `vision_tokens_used`, `cost_usd`

**Pending Fixes:**
- ⚠️ `processing_time_ms` - Code bug (timing measured after AI processing, not during)

---

## 1. `processing_time_ms` - Why Sometimes Low Values?

### Status: ✅ **TIMING IS CORRECT** (Audit Error - Fixed 2025-10-08)

**Previous Audit Claim:** Timing measured after AI processing (INCORRECT)

**Actual Code Flow:**
```typescript
// Pass1EntityDetector.ts line 80
const startTime = Date.now();  // ← START: Before AI call

// Line 99: THE EXPENSIVE AI CALL (3-5 minutes)
const aiResponse = await this.callAIForEntityDetection(input);

// Lines 102-131: Translation, validation, stats (~instant)

// Line 134: END TIMING (includes everything)
const processingTimeMs = Date.now() - startTime;

// Line 145: Pass to database builder
buildPass1DatabaseRecords(..., processingTimeMs);
```

**What it measures:** Full processing time including:
- AI vision processing (3-5 minutes) ✅
- Translation to database format (~instant) ✅
- Validation (~instant) ✅
- Statistics generation (~instant) ✅

### Why Low Values Occur:

If you see low values (0-1ms), possible causes:
1. **Test mode** - Mocked AI responses
2. **Cached responses** - Development environment
3. **Fast model** - GPT-5-mini on simple documents

### Verdict:
✅ **CORRECT** - Timing captures full AI processing duration

---

## 2. `confidence_distribution` - Purpose and Value

### What it is:
Breakdown of entity confidence levels

### Database Example:
```json
{
  "low": 0,      // Confidence < 0.6
  "medium": 0,   // Confidence 0.6-0.79
  "high": 40     // Confidence ≥ 0.8
}
```

### How it's Calculated:
```typescript
// pass1-database-builder.ts lines 264-280
for (const entity of entityAuditRecords) {
  if (entity.pass1_confidence >= 0.8) {
    distribution.high++;
  } else if (entity.pass1_confidence >= 0.6) {
    distribution.medium++;
  } else {
    distribution.low++;
  }
}
```

### Purpose:
- **Quality metric:** Shows overall extraction quality at a glance
- **Manual review queue:** Low-confidence entities flagged for human review
- **Model comparison:** Compare confidence across AI models
- **Cost/quality tradeoff:** Higher confidence = fewer manual reviews needed

### Use Cases:
```sql
-- Find sessions with low-quality extractions
SELECT * FROM pass1_entity_metrics
WHERE (confidence_distribution->>'low')::int > 5;

-- Track confidence trends over time
SELECT
  DATE_TRUNC('day', created_at),
  AVG((confidence_distribution->>'high')::int) as avg_high_confidence
FROM pass1_entity_metrics
GROUP BY 1;
```

### Verdict:
✅ **KEEP** - Valuable quality metric for monitoring and optimization

---

## 3. `cost_usd` - Source and Calculation (Options for Dynamic Pricing)

### Current Implementation:

**BACKEND CALCULATED** with hardcoded pricing:

```typescript
// Pass1EntityDetector.ts lines 508-524
private calculateCost(usage: any, fileSizeBytes: number): number {
  const GPT4O_PRICING = {
    input_per_1m: 2.50,      // ← HARDCODED for GPT-4o
    output_per_1m: 10.00,    // ← HARDCODED for GPT-4o
    image_per_1m: 7.65,      // ← HARDCODED for GPT-4o
  };

  const inputCost = (promptTokens / 1_000_000) * GPT4O_PRICING.input_per_1m;
  const outputCost = (completionTokens / 1_000_000) * GPT4O_PRICING.output_per_1m;
  const imageCost = (imageTokens / 1_000_000) * GPT4O_PRICING.image_per_1m;

  return inputCost + outputCost + imageCost;
}
```

### Current Problem:
- Using GPT-4o pricing for GPT-5-mini runs
- Cost estimates inflated ~5x (GPT-5-mini is ~5x cheaper)
- Will be forgotten when switching models

### Options for Dynamic Pricing:

#### Option 1: Remove `cost_usd` Column Entirely ✅ RECOMMENDED

**Rationale:**
- Token counts already stored (`vision_tokens_used`)
- Pricing changes frequently (models, providers, volume discounts)
- Can calculate cost on-demand with latest pricing
- No risk of stale/incorrect cost data

**Implementation:**
```sql
-- Remove column
ALTER TABLE pass1_entity_metrics DROP COLUMN cost_usd;

-- Calculate on-demand with latest pricing
SELECT
  vision_tokens_used,
  (vision_tokens_used / 1000000.0 * :current_model_price) as estimated_cost
FROM pass1_entity_metrics;
```

**Pros:**
- No hardcoded pricing to maintain
- Always use latest rates
- Easier model switching

**Cons:**
- Historical costs not preserved (but pricing changes anyway)
- Need pricing lookup when displaying costs

---

#### Option 2: Model-Specific Pricing Map

**Implementation:**
```typescript
const MODEL_PRICING = {
  'gpt-4o': {
    input_per_1m: 2.50,
    output_per_1m: 10.00,
    image_per_1m: 7.65
  },
  'gpt-5-mini': {
    input_per_1m: 0.15,   // ← Need to verify actual pricing
    output_per_1m: 0.60,
    image_per_1m: 0.42
  },
  'claude-3-opus': {
    input_per_1m: 15.00,
    output_per_1m: 75.00,
    image_per_1m: 0  // No image pricing
  }
};

const pricing = MODEL_PRICING[modelName] || MODEL_PRICING['gpt-4o'];
```

**Pros:**
- Historical costs preserved at time of processing
- Works offline
- Fast (no API calls)

**Cons:**
- Still hardcoded (just more complete)
- WILL be forgotten when adding new models
- Pricing changes require code updates

---

#### Option 3: Environment Variable Pricing

**Implementation:**
```typescript
// .env
GPT4O_INPUT_PRICE_PER_1M=2.50
GPT4O_OUTPUT_PRICE_PER_1M=10.00
GPT5_MINI_INPUT_PRICE_PER_1M=0.15
GPT5_MINI_OUTPUT_PRICE_PER_1M=0.60

// Code
const modelPrefix = modelName.replace(/[^a-z0-9]/gi, '_').toUpperCase();
const inputPrice = parseFloat(process.env[`${modelPrefix}_INPUT_PRICE_PER_1M`]);
const outputPrice = parseFloat(process.env[`${modelPrefix}_OUTPUT_PRICE_PER_1M`]);
```

**Pros:**
- Easy to update without code changes
- Per-environment pricing (dev vs prod)
- Centralized config

**Cons:**
- Still manual updates needed
- More env vars to manage
- Easy to miss when adding models

---

#### Option 4: Live API Pricing Lookup ❌ NOT RECOMMENDED

**Implementation:**
```typescript
// Fetch from OpenAI pricing API (if it exists)
const pricing = await fetch('https://api.openai.com/v1/pricing');
```

**Pros:**
- Always accurate
- No maintenance

**Cons:**
- OpenAI has NO public pricing API
- Would need web scraping (fragile, TOS violation)
- Network dependency for every document
- Rate limits, failures, latency
- **NOT VIABLE**

---

#### Option 5: AI Calculates Own Cost ❌ NOT RECOMMENDED

**Prompt addition:**
```typescript
"Also calculate and return your own processing cost based on:
- Your input tokens: {prompt_tokens}
- Your output tokens: {completion_tokens}
- Your current pricing rates
Return as: { cost_usd: 0.123 }"
```

**Pros:**
- AI knows its own pricing (theoretically)

**Cons:**
- Wastes output tokens asking AI to do math
- AI doesn't have access to its own pricing
- AI may hallucinate costs
- Unreliable and wasteful
- **NOT VIABLE**

---

### Recommendation:

**Option 1: REMOVE `cost_usd` column**

**Rationale:**
1. Token counts are the source of truth (already stored)
2. Pricing changes frequently - stored costs become stale
3. Calculate on-demand with latest pricing when needed
4. No risk of forgotten updates when switching models
5. Simpler architecture

**Implementation:**
```sql
-- Remove column
ALTER TABLE pass1_entity_metrics DROP COLUMN cost_usd;

-- Store pricing config separately (optional)
CREATE TABLE ai_model_pricing (
  model_name TEXT PRIMARY KEY,
  input_price_per_1m DECIMAL,
  output_price_per_1m DECIMAL,
  image_price_per_1m DECIMAL,
  effective_date TIMESTAMP,
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Calculate cost on-demand
SELECT
  m.*,
  p.input_price_per_1m * (m.vision_tokens_used / 1000000.0) as estimated_cost
FROM pass1_entity_metrics m
LEFT JOIN ai_model_pricing p ON p.model_name = m.vision_model_used;
```

**Alternative if you must keep costs:**
Use Option 2 (model pricing map) with TODO comments and runtime warnings:
```typescript
// TODO: Update pricing when adding new models
// WARNING: Pricing was last updated 2025-10-08
if (!MODEL_PRICING[modelName]) {
  console.error(`⚠️  No pricing for model ${modelName} - using GPT-4o pricing as fallback`);
  // Alert/log to catch missing pricing
}
```

---

## 4. `user_agent` & `ip_address` - Why NULL and Original Purpose?

### Current State:
Always `NULL` in all runs

### Why They Were Added:

**Original commit:** ff0927a (2025-10-03) - "Implement Pass 1 Entity Detection Module"

These fields were added in the **initial architecture design** as part of a **comprehensive audit trail** for healthcare compliance:

**Intended Purpose:**
- `user_agent` - Track which client/device initiated processing (web, mobile, API)
- `ip_address` - Security audit trail for HIPAA/compliance requirements

**Healthcare Compliance Context:**
- HIPAA requires tracking "who accessed what, when, from where"
- IP address = "from where"
- User agent = device/client identification
- Part of comprehensive access logging

### Why They're NULL:

**Current Architecture Reality:**
```
User uploads file → Supabase Storage → Background worker processes
                                        ↑ No user session context here
```

The worker has no access to:
- HTTP request headers (no user-agent)
- Client IP address
- User session data

### Should We Remove Them?

**Arguments FOR Removal:**
- Background worker has no user context (can't populate)
- Already have `profile_id` for ownership tracking
- Upload logging happens at Supabase Storage level (separate audit)
- Not used in 100% of runs (always NULL)

**Arguments AGAINST Removal:**
- **Healthcare compliance design intent** - part of comprehensive audit trail
- **Future-proofing** - if we add direct API processing (non-background)
- **Audit completeness** - even if NULL, documents that field was considered

### Alternative Approaches:

**Option 1: Populate from Upload Metadata**
```typescript
// When user uploads file, capture metadata
const uploadMetadata = {
  user_agent: req.headers['user-agent'],
  ip_address: req.ip,
  uploaded_at: new Date()
};

// Store in shell_files table
// Pass to worker via job metadata
// Worker populates pass1_entity_metrics
```

**Option 2: Keep as NULL with Documentation**
```typescript
// pass1-types.ts
user_agent?: string;  // NULL for background jobs; populated for direct API calls
ip_address?: string;  // NULL for background jobs; captured at upload time
```

**Option 3: Move to Upload Audit Table**
```sql
-- Separate table for upload events (where user context exists)
CREATE TABLE file_upload_audit (
  shell_file_id UUID PRIMARY KEY,
  user_agent TEXT,
  ip_address TEXT,
  uploaded_by UUID REFERENCES user_profiles(id),
  uploaded_at TIMESTAMP
);

-- Remove from pass1_entity_metrics (processing context has no user session)
```

### Recommendation:

**Keep fields but add documentation:**

1. **Rename for clarity:**
   ```sql
   ALTER TABLE pass1_entity_metrics
     RENAME COLUMN user_agent TO initiating_user_agent;
   ALTER TABLE pass1_entity_metrics
     RENAME COLUMN ip_address TO initiating_ip_address;

   COMMENT ON COLUMN pass1_entity_metrics.initiating_user_agent IS
     'User agent of client that initiated processing (NULL for background jobs)';
   COMMENT ON COLUMN pass1_entity_metrics.initiating_ip_address IS
     'IP address of client that initiated processing (NULL for background jobs)';
   ```

2. **Populate when possible:**
   - If future direct API processing: populate from request headers
   - If background worker: leave NULL (document why)
   - If upload event tracking needed: store in separate upload audit table

3. **Compliance value:**
   - Even if NULL, shows field was considered for audit trail
   - Documents that processing context has no user session
   - Maintains healthcare audit completeness

**Verdict:** ✅ **KEEP with documentation** - Part of compliance audit design, even if NULL in current architecture

---

## Summary Table - Recommendations

| Column | Issue | Verdict | Recommendation |
|--------|-------|---------|----------------|
| `processing_time_ms` | Always 0 (timing bug) | ⚠️ FIX | Move `startTime` to before AI call |
| `confidence_distribution` | Working correctly | ✅ KEEP | Valuable quality metric |
| `cost_usd` | Hardcoded pricing, forgotten updates | ❌ REMOVE | Use tokens + pricing lookup on-demand |
| `user_agent` | NULL (no user context) | ✅ KEEP | Compliance audit design, document NULL reason |
| `ip_address` | NULL (no user context) | ✅ KEEP | Compliance audit design, document NULL reason |

### Priority Actions:

1. **Fix `processing_time_ms` bug** (move timing to before AI call)
2. **Remove `cost_usd` column** (calculate on-demand from tokens)
3. **Document `user_agent`/`ip_address` NULL behavior** (compliance design intent)

---

## CRITICAL ISSUE: Token Breakdown Data Loss 🚨

### Discovery Date: 2025-10-08

### Problem Statement

**We capture detailed token breakdown from OpenAI API but only store the total, making accurate cost analysis impossible.**

### Evidence

#### What OpenAI API Returns:
```typescript
// Pass1EntityDetector.ts lines 349-352
token_usage: {
  prompt_tokens: response.usage?.prompt_tokens || 0,      // Text + image input tokens
  completion_tokens: response.usage?.completion_tokens || 0,  // AI output tokens
  total_tokens: response.usage?.total_tokens || 0,       // prompt + completion
  image_tokens: this.estimateImageTokens(optimizedSize), // WE ESTIMATE (not from API!)
}
```

#### What We Store:
```typescript
// pass1-database-builder.ts line 258
vision_tokens_used: aiResponse.processing_metadata.token_usage.total_tokens  // TOTAL ONLY ❌
```

**Result:** Input/output breakdown is **LOST** after processing

### Why Critical

**Different pricing for input vs output:**
- GPT-4o: Input $2.50/1M, Output $10.00/1M (4x difference)
- GPT-5-mini: Input $0.15/1M, Output $0.60/1M (4x difference)

**Without breakdown:**
❌ Can't recalculate costs with new pricing
❌ Can't optimize input vs output separately
❌ Can't track cost attribution accurately

### Image Token Clarification

**Your question: Are image_tokens input tokens alongside prompt_tokens?**

**Answer: YES, but with a twist:**

```
OpenAI API Response:
├── prompt_tokens = text input + image input (COMBINED by OpenAI)
├── completion_tokens = AI output
└── total_tokens = prompt + completion

Our code separately estimates:
└── image_tokens = (fileSize / 1000) * 85  ← Rough guess, NOT from API
```

**The confusion:**
1. OpenAI **doesn't separate** text vs image input tokens
2. Both are combined in `prompt_tokens`
3. We **estimate image tokens separately** based on file size
4. Our estimation is just a guess (~85 tokens/KB approximation)
5. **Real image tokens are already in `prompt_tokens`** - we're estimating what we already have!

**Bottom line:**
- `prompt_tokens` (from API) = total input (text + images combined)
- `image_tokens` (our calculation) = rough estimate for internal tracking
- We should just use `prompt_tokens` as-is for input cost

---

### Investigation Results

#### All Tables with Token/Cost Columns:

| Table | Current Columns | Issue | Priority |
|-------|----------------|-------|----------|
| **`pass1_entity_metrics`** | `vision_tokens_used` (total only) | ❌ No input/output breakdown | **HIGH** |
| **`pass2_clinical_metrics`** | `clinical_tokens_used` (total only) | ❌ No input/output breakdown | **HIGH** |
| **`pass3_narrative_metrics`** | `semantic_tokens_used` (total only) | ❌ No input/output breakdown | **HIGH** |
| `semantic_processing_sessions` | `token_usage_input`, `token_usage_output` | ✅ **HAS BREAKDOWN!** | ✅ GOOD |
| `entity_processing_audit` | `pass1_token_usage`, `pass1_image_tokens` | ❌ Duplicated session data | Already flagged |
| `ai_processing_summary` | `total_tokens_used` | ❌ No breakdown | MEDIUM |
| `narrative_creation_audit` | `ai_token_usage` (JSONB) | ⚠️ Unknown if has breakdown | LOW |
| `shell_file_synthesis_results` | `ai_token_usage` (JSONB) | ⚠️ Unknown if has breakdown | LOW |

**Good Example: `semantic_processing_sessions` already does it right!**
```sql
token_usage_input INTEGER,    -- Input tokens
token_usage_output INTEGER,   -- Output tokens
processing_cost_usd NUMERIC   -- Calculated from breakdown
```

#### Schema Files to Update:

**Source schemas (.md):**
- `bridge-schemas/source/pass-1/pass1_entity_metrics.md`
- `bridge-schemas/source/pass-2/pass2_clinical_metrics.md`
- `bridge-schemas/source/pass-3/pass3_narrative_metrics.md`

**Generated schemas (.json):**
- `bridge-schemas/detailed/pass1_entity_metrics.json`
- `bridge-schemas/detailed/pass2_clinical_metrics.json`
- `bridge-schemas/detailed/pass3_narrative_metrics.json`
- `bridge-schemas/minimal/[corresponding files]`

#### Code Files to Update:

**Pass 1:**
- `apps/render-worker/src/pass1/pass1-database-builder.ts`
- `apps/render-worker/src/pass1/pass1-types.ts`

**Pass 2 & 3:** (If they exist - need to check)
- `apps/render-worker/src/pass2/[files]`
- `apps/render-worker/src/pass3/[files]`

---

### Proposed Solution

#### Database Schema Changes:

```sql
-- Pass 1 Entity Metrics
ALTER TABLE pass1_entity_metrics
  ADD COLUMN input_tokens INTEGER,           -- prompt_tokens from API (text + images)
  ADD COLUMN output_tokens INTEGER,          -- completion_tokens from API
  RENAME COLUMN vision_tokens_used TO total_tokens;

-- Pass 2 Clinical Metrics
ALTER TABLE pass2_clinical_metrics
  ADD COLUMN input_tokens INTEGER,
  ADD COLUMN output_tokens INTEGER,
  RENAME COLUMN clinical_tokens_used TO total_tokens;

-- Pass 3 Narrative Metrics
ALTER TABLE pass3_narrative_metrics
  ADD COLUMN input_tokens INTEGER,
  ADD COLUMN output_tokens INTEGER,
  RENAME COLUMN semantic_tokens_used TO total_tokens;

-- Remove cost_usd from all (calculate on-demand)
ALTER TABLE pass1_entity_metrics DROP COLUMN cost_usd;
ALTER TABLE pass2_clinical_metrics DROP COLUMN cost_usd;
ALTER TABLE pass3_narrative_metrics DROP COLUMN cost_usd;
```

#### Code Changes:

```typescript
// pass1-database-builder.ts (and pass2, pass3 equivalents)
{
  total_tokens: aiResponse.processing_metadata.token_usage.total_tokens,
  input_tokens: aiResponse.processing_metadata.token_usage.prompt_tokens,      // From API
  output_tokens: aiResponse.processing_metadata.token_usage.completion_tokens, // From API
  // Remove: cost_usd (calculate on-demand)
  // Remove: image_tokens estimation (already in input_tokens)
}
```

#### Cost Calculation (On-Demand):

```typescript
// Calculate with latest pricing when needed
function calculateCost(tokens: { input: number, output: number }, modelName: string) {
  const pricing = MODEL_PRICING[modelName];
  return (tokens.input / 1_000_000 * pricing.input_per_1m) +
         (tokens.output / 1_000_000 * pricing.output_per_1m);
}
```

---

### Migration Strategy

**Phase 1: Add new columns (non-breaking)**
- Add `input_tokens`, `output_tokens` to metrics tables
- Existing records: NULL (historical data lost)
- New records: Full breakdown

**Phase 2: Update code**
- Store breakdown from API response
- Stop estimating image tokens separately

**Phase 3: Deprecate old columns**
- Rename `vision_tokens_used` → `total_tokens` (keep for compatibility)
- Remove `cost_usd` (calculate on-demand)

**Phase 4: Update documentation**
- Update schema .md files
- Regenerate .json schemas
- Document breaking changes

---

### Impact

**Data Loss:**
- ❌ Historical records: Cannot recover input/output breakdown
- ✅ Future records: Full breakdown available

**Backwards Compatibility:**
- ✅ Keep `total_tokens` (renamed)
- ✅ New columns nullable
- ✅ No breaking changes to existing queries

---

### Next Steps (Pending Your Approval)

1. ✅ **Document this issue** (DONE - this section)
2. ⏳ Update schema .md files
3. ⏳ Create migration script (following `migration_history/` convention)
4. ⏳ Update TypeScript interfaces
5. ⏳ Update database builder code
6. ⏳ Test and deploy

**Waiting for your approval before creating migration script.**

---

---

## 5. COMPLETE COLUMN INVENTORY - Final Audit (2025-10-08)

**Purpose:** Definitive review of ALL 18 columns in `pass1_entity_metrics` table

### PRIMARY KEYS & REFERENCES (4 columns)

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 1 | `id` | uuid | NO | ✅ KEEP | Primary key |
| 2 | `profile_id` | uuid | NO | ✅ KEEP | FK to user_profiles (data ownership) |
| 3 | `shell_file_id` | uuid | NO | ✅ KEEP | FK to shell_files (document reference) |
| 4 | `processing_session_id` | uuid | NO | ✅ KEEP | FK to ai_processing_sessions (audit trail) |

### PROCESSING METRICS (6 columns)

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 5 | `entities_detected` | integer | NO | ✅ KEEP | Count of entities found in document |
| 6 | `processing_time_ms` | integer | NO | ⚠️ KEEP (BUG) | Session processing duration - **needs code fix** (currently measures DB building, not AI processing) |
| 7 | `vision_model_used` | text | NO | ✅ KEEP | AI model identifier (e.g., "gpt-5-mini") |
| 8 | `ocr_model_used` | text | YES | ✅ KEEP | OCR provider (google_vision, aws_textract) |
| 9 | `ocr_agreement_average` | numeric | YES | ✅ KEEP | Average AI-OCR agreement score (0.0-1.0) |
| 10 | `ocr_pages_processed` | integer | YES | ✅ KEEP | Number of pages processed by OCR |

### QUALITY METRICS (2 columns)

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 11 | `confidence_distribution` | jsonb | YES | ✅ KEEP | Breakdown of confidence levels (low/medium/high) - valuable quality metric |
| 12 | `entity_types_found` | text[] | YES | ✅ KEEP | Array of entity subtypes detected |

### TOKEN BREAKDOWN (3 columns) - Migration 15 ✅

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 13 | `input_tokens` | integer | YES | ✅ KEEP | Input tokens (text + images) from OpenAI API |
| 14 | `output_tokens` | integer | YES | ✅ KEEP | Output tokens (AI completion) from OpenAI API |
| 15 | `total_tokens` | integer | YES | ✅ KEEP | Sum of input + output tokens |

**Migration 15 Changes:**
- ✅ REMOVED: `vision_tokens_used` (replaced by `total_tokens`)
- ✅ REMOVED: `cost_usd` (calculate on-demand from token breakdown)
- ✅ ADDED: `input_tokens`, `output_tokens` for accurate cost calculation

### COMPLIANCE AUDIT FIELDS (2 columns)

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 16 | `user_agent` | text | YES | ✅ KEEP | Client identifier (NULL for background jobs) - compliance design |
| 17 | `ip_address` | inet | YES | ✅ KEEP | Source IP (NULL for background jobs) - compliance design |

**Note:** These fields are NULL in current architecture (background worker has no user context). Kept for:
- Future direct API processing capability
- Healthcare compliance audit trail completeness
- Documents that field was considered even if unpopulated

### AUDIT TIMESTAMPS (1 column)

| # | Column | Type | Nullable | Verdict | Reason |
|---|--------|------|----------|---------|--------|
| 18 | `created_at` | timestamptz | YES | ✅ KEEP | Session creation timestamp |

---

## FINAL VERDICT SUMMARY

**Total Columns:** 18
**Keep:** 18 ✅
**Remove:** 0 ❌

**Previously Removed (Migration 15):**
- ✅ `vision_tokens_used` - Replaced by `total_tokens` with input/output breakdown
- ✅ `cost_usd` - Calculate on-demand from accurate token breakdown

**Pending Code Fixes (Not Removals):**
- ⚠️ `processing_time_ms` - Move timing to BEFORE AI call (currently measures DB building only)

**Cost Calculation (On-Demand):**
```sql
-- Accurate cost calculation with token breakdown
SELECT
  shell_file_id,
  input_tokens,
  output_tokens,
  total_tokens,
  -- GPT-5-mini pricing example
  (input_tokens / 1000000.0 * 0.15) +
  (output_tokens / 1000000.0 * 0.60) as cost_usd
FROM pass1_entity_metrics;
```

**Architecture Status:** ✅ **CLEAN** - Accurate token breakdown enables precise cost analysis and model optimization.

---

**Last Updated:** 2025-10-08
