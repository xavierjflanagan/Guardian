# Test 06 v2.1: Testing Improved Prompt with Metadata Guidance

**Date Created:** November 2, 2025
**v2.1 Deployed:** 10:35:34 AM UTC
**Purpose:** Compare boundary detection accuracy between v1 (original) and v2.1 (context-based metadata guidance)

---

## Quick Summary

**v2.1 is LIVE and ready for testing**

### What Changed
- **v1 Prompt:** No metadata page guidance
- **v2.1 Prompt:** Context-based metadata recognition using provider/facility continuity

### Test File
```
Location: /Users/xflanagan/Documents/GitHub/Guardian-Cursor/sample-medical-records/patient-006-emma-thompson/pdfs/
Filename: 006_Emma_Thompson_Frankenstein_Progress_note_Emergency_summary.pdf
Size: 758 KB
Pages: 20
```

### Expected Improvement
- **v1 Result:** Boundary at page 11/12 (4 pages off)
- **v2.1 Expected:** Boundary at page 13/14 (correct)
- **Key Fix:** Pages 12-13 should be grouped with Progress Note (Mara Ehret), not Emergency Summary (Matthew Tinkham)

---

## Testing Instructions

### Step 1: Upload Test File

1. **Navigate to:** https://exorahealth.com.au
2. **Log in as:** Emma Thompson patient account
3. **Upload file:** `006_Emma_Thompson_Frankenstein_Progress_note_Emergency_summary.pdf`
4. **Wait for processing:** ~48 seconds (based on v1 timing)

### Step 2: Get Shell File ID

After upload completes, run this query to get the new shell_file_id:

```sql
-- Get the newest Frankenstein test file
SELECT
  sf.id as shell_file_id,
  sf.filename,
  sf.created_at,
  sf.pass_0_5_completed,
  COUNT(he.id) as encounter_count
FROM shell_files sf
LEFT JOIN healthcare_encounters he ON he.primary_shell_file_id = sf.id
WHERE sf.patient_id = 'd1dbe18c-afc2-421f-bd58-145ddb48cbca'
  AND sf.filename LIKE '%Frankenstein%'
GROUP BY sf.id, sf.filename, sf.created_at, sf.pass_0_5_completed
ORDER BY sf.created_at DESC
LIMIT 2;
```

Expected output:
```
Row 1: v2.1 test (newest, created after 10:35:34 AM)
Row 2: v1 test (shell_file_id: e4a19fe4-bf22-4c7a-b915-e0cf2b278c21, created at 09:18:59 AM)
```

### Step 3: Compare Results

Use the comparison query below (replace `<NEW_SHELL_FILE_ID>` with actual value from Step 2):

```sql
-- Compare v1 vs v2.1 encounter detection
WITH v1_encounters AS (
  SELECT
    'v1' as version,
    encounter_type,
    page_ranges,
    provider_name,
    facility_name,
    encounter_date,
    confidence
  FROM healthcare_encounters
  WHERE primary_shell_file_id = 'e4a19fe4-bf22-4c7a-b915-e0cf2b278c21'
  ORDER BY (page_ranges::jsonb->0->>0)::int
),
v2_encounters AS (
  SELECT
    'v2.1' as version,
    encounter_type,
    page_ranges,
    provider_name,
    facility_name,
    encounter_date,
    confidence
  FROM healthcare_encounters
  WHERE primary_shell_file_id = '<NEW_SHELL_FILE_ID>'
  ORDER BY (page_ranges::jsonb->0->>0)::int
)
SELECT * FROM v1_encounters
UNION ALL
SELECT * FROM v2_encounters
ORDER BY version, (page_ranges::jsonb->0->>0)::int;
```

---

## Expected Results Analysis

### Scenario A: v2.1 Fixes Boundary (SUCCESS)

**v1 Results:**
- Encounter 1: Pages 1-11 (Mara Ehret)
- Encounter 2: Pages 12-20 (Matthew Tinkham)

**v2.1 Expected:**
- Encounter 1: Pages 1-13 (Mara Ehret) ← Fixed! Now includes metadata pages 12-13
- Encounter 2: Pages 14-20 (Matthew Tinkham) ← Correct start

**Interpretation:** Context-based metadata guidance worked! Pages 12-13 (with Mara Ehret signatures) correctly grouped with Progress Note.

### Scenario B: No Change (PROMPT STILL NEEDS WORK)

**v2.1 Results:**
- Encounter 1: Pages 1-11
- Encounter 2: Pages 12-20

**Interpretation:** v2.1 prompt didn't improve boundary detection. Next steps:
1. Review GPT-5-mini output reasoning (if available in logs)
2. Consider upgrading to GPT-5 for better reasoning
3. Further refine prompt with more explicit examples

### Scenario C: Worse Results (UNEXPECTED REGRESSION)

**v2.1 Results:**
- 3+ encounters detected
- OR different boundary (not 11/12 or 13/14)

**Interpretation:** v2.1 prompt introduced confusion. Revert to v1 and redesign.

---

## Validation Checklist

After running the comparison query, verify:

- [ ] **Encounter count:** Both v1 and v2.1 detect exactly 2 encounters
- [ ] **Encounter types:** Both detect `specialist_consultation` + `emergency_department`
- [ ] **Providers:** Both extract "Mara B Ehret, PA-C" and "Matthew T Tinkham, MD"
- [ ] **Facilities:** Both extract correct facility names
- [ ] **Dates:** Both extract Oct 27, 2025 and Jun 22, 2025
- [ ] **Confidence scores:** Both have 90%+ confidence
- [ ] **Boundary location:** v2.1 boundary at page 13/14 (improved from v1's 11/12)

---

## Success Criteria

**Pass 0.5 v2.1 is successful if:**

1. **Boundary improved:** v2.1 detects boundary at page 13/14 (vs v1's 11/12)
2. **No regressions:** All other metrics (encounter count, types, providers) remain accurate
3. **Confidence maintained:** Confidence scores remain 90%+
4. **Processing time similar:** ~40-50 seconds total

**If v2.1 passes all criteria:** Deploy to production, document in RESULTS.md

**If v2.1 fails:** Proceed with testing GPT-5 (more expensive but stronger reasoning)

---

## Cost Comparison

### Current v1/v2.1 (GPT-5-mini)
- Model: gpt-5-mini-2025-08-07
- Cost per 20-page doc: $0.0074 (AI) + $0.0300 (OCR) = $0.0374
- Annual (100K docs): $7,400

### Potential GPT-5 Upgrade
- Model: gpt-5-2025-08-07
- Cost per 20-page doc: ~$0.08-0.15 (AI) + $0.0300 (OCR) = ~$0.11-0.18
- Annual (100K docs): ~$11,000-18,000
- Cost increase: ~$3,600-10,600/year (+48% to +143%)

---

## Database Reference

### v1 Original Test (Shell File ID: e4a19fe4-bf22-4c7a-b915-e0cf2b278c21)

```sql
-- Get v1 encounters
SELECT * FROM healthcare_encounters
WHERE primary_shell_file_id = 'e4a19fe4-bf22-4c7a-b915-e0cf2b278c21'
ORDER BY created_at;

-- Get v1 metrics
SELECT * FROM pass05_encounter_metrics
WHERE shell_file_id = 'e4a19fe4-bf22-4c7a-b915-e0cf2b278c21';
```

**v1 Results:**
- Encounter 1: specialist_consultation, pages [[1,11]], Mara B Ehret, 95% confidence
- Encounter 2: emergency_department, pages [[12,20]], Matthew T Tinkham, 94% confidence

---

## Next Steps After Testing

### If v2.1 Succeeds
1. Update RESULTS.md with v2.1 outcomes
2. Document prompt improvement impact
3. Move to Test 07 (threshold discovery with 200+ pages)
4. Consider v2.1 production-ready

### If v2.1 Fails
1. Test with GPT-5 (change one line in `encounterDiscovery.ts:72`)
2. Compare GPT-5 results vs v2.1
3. Decide: Better prompt with GPT-5-mini OR accept higher cost for GPT-5?

---

## Related Files

- **v1 Prompt Backup:** `apps/render-worker/src/pass05/aiPrompts.v1.ts`
- **v2.1 Current Prompt:** `apps/render-worker/src/pass05/aiPrompts.ts`
- **Model Selection:** `apps/render-worker/src/pass05/encounterDiscovery.ts:72`
- **v1 Test Results:** `RESULTS.md` and `BOUNDARY_ISSUE_ANALYSIS.md`

---

**Status:** Ready for testing
**v2.1 Deployment:** LIVE (Render deploy ID: dep-d43j71k9c44c73atbg0g)
**Last Updated:** November 2, 2025
