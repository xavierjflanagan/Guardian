# Pass 1 & 2 Strategy-A Design Notes

**Author:** Xavier Flanagan
**Date:** 28th November 2025
**Type:** Historical design notes and conversation log
**Status:** IMPLEMENTED (Pass 1) / NOT STARTED (Pass 2)

---

## Document Purpose

This file contains the original design thinking and decision-making conversation that led to Pass 1 Strategy-A implementation. It is preserved as a historical record of architectural decisions.

**For current implementation status:** See `strategy-a/PASS1-STRATEGY-A-MASTER.md`

**Key sections in this document:**
- Lines 1-52: Original Strategy-A vision for Pass 1 and Pass 2
- Lines 186-323: Safe-split points decision (Option B2 chosen)
- Lines 346-452: Batch failure retry logic design

---

## Original Design Vision and subsequent discussion log

Xavier - 28th November 2025

New strategy a design for pass 1:
  1. Pass 1 will receive at most 1 encounter to analyse per api call (encounter discovery performed in pass 0.5). 
    - Each api call will recieve enchanced ocr output containing the text + y coordinates only.
    - If the 'complete' encounter (post pending encounter reconciliation) is high volume (or the economomics and UX -processing time - indicate it) the encounter will be broken up into batches via the safe-split points that were generated by pass05. 
        - If batched, each pass 1 api call batch ought to recieve 
            - a brief context package that provides context about the encounter within which the batch resides. -- But does it need this?? what benefit does this add?  might not be needed
            - The enhanced ocr output specific to the batch, containing the text + y coordinates only
  2. For a given api call batch, the purspoe and tasks of Pass 1 are:
    a. Identify bridge scehmas for pass 2
      - Bridge scehmas are needed by pass 2 to correctly extract all necessary data in the correct format as per db schemas.
      - But a secondary task here for pass 1 could be to go a little deeper and categorize ocr data into 'bridge schemas regions'. Where y to y coordinate zones are marked and recorded. 
        - This feature would fascilitate efficient batching by 'bridge schema zone'. 
        - For example, in a 20 page file with 5 seperate path result sections scattered throughout the 20 pages. If they are all identified as a 'results' 'bridge schema zone' then they can all be grouped together and sent to their own pass 2 api call that requires only the 1 bridge schema AND runs in parralel to other batched zones. Furthermore, we could parralel all 5 seperate path zones and cache the prompt + results bridge schema, meaning the processing time is 4 times as fast with only a minor increase in token cost (caching reduces cost by 90%). This architectural design would not only speed up pass 2 processing whihc is invaluable to UX, but also potentiially allow us to use cheaper ai models as the computation and reasoning and context volumes will be lower. 
    b. Extract all clincial entities that need a medical code shortlist assigned to them by pass 1.5 (pass 1.5 shorltist for each clincial entity handed across to pass2 for selection during pass 2 ai call)
        - The information that the pass 1 ai needs to output is only that which is necessary for the pass1.5 module to successfully function and generate the best possible shortlist of medical codes for each clincial entity, to hand across to pass 2. 
        - atm the pass 1.5 universal_medical_codes table has the the entity_type column that i believe has the following options;
                'medication'::character varying,
                'condition'::character varying,
                'procedure'::character varying,
                'observation'::character varying,
                'allergy'::character varying,
                'lab_result'::character varying,
                'vital_sign'::character varying,
                'physical_finding'::character varying
        - so all pass1 really needs to do is identify the entity (original text), the entity_type so that we can easily do some entity_type filtering to make pass1.5's job easier, before lexial and semantic vector matching/short-listing occurs. 
        - But also, it would be a good idea for pass1 to output some additional alias names or synonym names to further help pass1.5 in ase the original text fo the extracted entity is alternative or spelling mistake or rare brand name or region specific name etc. (but maybe cap it to 3 alias 1 word texts to avoid output token bloat). 

Then pass 2 will take over:
  1. Pass 2 will analyse at most 1 encounter per api call (encounters discovery performed in pass 0.5). 
    - If the encounter is high volume (or the economomics and UX -processing time - indicate it) the encounter will be broken up into inteligently determined batches, primarily based on the identified bridge schema zones. 
    - For a given api call batch, pass 2 will receive 
      a. A brief context package that provides context about the encounter within which the batch, and clinical entities, reside.
          - Not sure how helpful to the pass 2 ai this will be, or whether it will alter the quality and accuracy of the output at all, but worth including for now. 
      b. The enhanced ocr output specific to the batch, containing the text + xy coordinates for each word within the batch
          - xy coordinates (via the enhanced-xy-ocr format) are needed to:
            - Assign bounding boxes to each clinical entity. To reduce token output by 50% the ai need only output 1 y and 1 x. With a post-ai cross-analysis function to find the other two vertices
            - Allow the pass 2 ai model to 'see' the layout of strangely formatted text, such as path result tables with lateral columns. This is the real reason why xy coordinates are needed for pass 2.
      c. The specific bridge scehma to help guide the enrichment process and output requiremnts 
      d. A list of all the pass 1 clinical entities (original text markers) that it must 'enrich'
          - Each clinical entity will come with a shortlist of matched medical codes (~ 10-20 maybe). 
  2. Pass 2 will also have a second side alley system for any non-text blocks identified and labeled within the ocr output data, such as non-text images, graphs etc. 
    - If this second path is triggered by the presence of non-text blocks, then the cropped pixels will be sent to accompany the enhanced xy ocr output within the pass 2 api call, for the given batch. 
    - For now pass 1 does not need to be involved at all in this second side alley non-text block pathway, and hopefully pass 2 can deal with it itself without the prep material that pass 1 provides it. 
        

End outcome: 
  - Every clinical entity from the uploaded file is extracted labeled and stored inteligently, with all context recorded and preserved, with all clincial entitiy relationships and encoutner-clincial-entity relationship links preserved and utilized come frontend display and user interacation. 
  - The above relationships will be used for pass 3 Narrative generation (future concept). 
  - Future vision: Complety change the way users and doctors view and keep track of healthcare, replacing a document-centric or document-reliant approach to more meaningul narrative approach. Instead of referring to a document or file you refer to the narrative, which only used the file once off to gather and learn the ifnormation to build and bolster the health narrative and sub narratives. 







  claude code response 28th Nov 2025:
  I've read through your comprehensive design document. Here's my understanding:

  Core Vision:
  - Pass 1 becomes a lightweight "entity finder" - just identifies and classifies entities with minimal
   output (~2,700-3,800 tokens vs 22,000+)
  - Pass 2 becomes the "heavy lifter" - enriches only high-priority entities with full clinical
  extraction
  - Both passes use OCR-only (no vision/image tokens)
  - Chunk-based progressive processing from Pass 0.5 carries through

  Key Design Points I Captured:

  1. Pass 1 Output Schema - Minimal fields only:
    - entity_id, original_text, entity_category, entity_subtype
    - y_coordinate (not full bbox), page_number, chunk_id
    - confidence, unique_marker
    - No visual_interpretation, no ocr_cross_reference, no quality_indicators blocks
  2. Priority System - Pass 1 assigns priority (P1-P4) to determine what Pass 2 processes:
    - P1: Critical clinical (diagnoses, medications, allergies)
    - P2: Important clinical (vitals, lab results)
    - P3: Context (providers, facilities)
    - P4: Structure (headers, footers) - Skip Pass 2
  3. Chunk-Aware Processing - Entities inherit chunk_id from Pass 0.5, enabling progressive enrichment
  4. Database Simplification - Many current Pass 1 tables become obsolete or simplified

  ---
  My Plan for the Master File:

  Create: shared/docs/architecture/database-foundation-v3/ai-processing-v3/implementation-planning/pass
  -1-entity-detection/strategy-a/PASS1-STRATEGY-A-MASTER.md

  Structure:
  1. OVERVIEW (brief - links to your design doc for full context)

  2. FILE ORGANIZATION
     - Documentation structure
     - Code structure (apps/render-worker/src/pass1-v2/)
     - Schema locations

  3. EXISTING ASSETS INVENTORY
     - Scripts to archive (old pass1/*.ts)
     - Scripts to reuse (e.g., retry.ts, logger.ts)
     - Tables to deprecate vs keep
     - Schemas to model from

  4. NEW COMPONENT DESIGNS
     - Pass 1 prompt (minimal)
     - Pass 1 output schema
     - Database table(s)
     - TypeScript types
     - Worker integration points

  5. IMPLEMENTATION CHECKLIST
     [Single source of truth - each item has: status checkbox, description, notes/blockers]
     - [ ] Create pass1-v2 folder structure
     - [ ] Design new entity_detections table
     - [ ] Write minimal prompt
     - [ ] etc...

  Questions Before I Create:

  1. Folder naming: Should new Pass 1 code go in pass1-v2/ or replace pass1/ directly? (I lean toward
  pass1-v2/ to keep old code as reference during build)
  2. Database approach: New table like pass1_entity_detections or modify existing
  entity_processing_audit? (I lean toward new table given the schema is so different)
  3. Your design doc location: Should I reference it inline or keep it separate? It's currently at
  pass-1-and-2-strategy-a-design-Xavier-28-11-2025.md
  4. Pass 2 scope: Include Pass 2 in this master file or separate master file? (I lean toward separate
  since it's a different implementation phase)

  Does this approach match what you want?



  > aswers to your questions: 1. place 'new Pass 1 code go in pass1-v2/' so create new folder.
    2. create new table and i like your name suggestions, if we ever create any new tables 
for pass 1 it would be nice for human readability reaasons to include pass1 int he title, as
 you have done.   3. Keep it seperate for now but also re-phrase and rework it nicely into 
the start of the file as part of the overivew or vision etc.    4. Keep pass 2 seperate for 
now, but dont be afraind to mention and refernce pass 2 designs if their crucial to the 
understand and mechanisms of pass 1.     5. and my own question; what do you mean by 
"Entities inherit chunk_id from Pass 0.5" ?  This sounds important because im worried you're
 not focusing on the part of the pipeline that i described as "Pass 1 will receive at most 1
 encounter to analyse per api call (encounters discovery performed in pass 0.5). 
    - If the encounter is high volume (or the economomics and UX -processing time - indicate
 it) the encounter will be broken up into batches via the safe-split points that were 
generated by pass05." 
  ⎿ ⧉ Selected 2 lines from 
    shared/docs/architecture/database-foundation-v3/ai-processing-v3/implementation-planning/pas
    s-1-entity-detection/pass-1-and-2-strategy-a-design-Xavier-28-11-2025.md in Cursor

⏺ You're right to flag that - I misunderstood the granularity.

  My incorrect assumption: Pass 1 receives a chunk (arbitrary text segment) and finds
  entities within it.

  Your actual design: Pass 1 receives one encounter (already discovered by Pass 0.5) and
  extracts entities from that encounter's content. If the encounter is large, it gets split
  at safe-split points into batches.

  The key difference:
  - Pass 0.5 does: OCR → chunk processing → encounter discovery → safe-split point
  identification
  - Pass 1 receives: A single pending_encounter (or batch of one encounter's content if
  split)
  - Pass 1 does NOT receive raw chunks - it receives encounter-scoped content

  So the flow is:
  Pass 0.5 output: pending_encounters table (each row = one discovered encounter)
                   ↓
  Pass 1 input:    One pending_encounter at a time
                   (with its text content, Y-coordinates, safe-split points if needed)
                   ↓
  Pass 1 output:   Entities belonging to that specific encounter

  This means:
  - Entities don't inherit chunk_id - they inherit pending_encounter_id (or final
  encounter_id after reconciliation)
  - The job queue dispatches Pass 1 jobs per encounter, not per document or per chunk
  - If encounter is large (e.g., 10-page hospital discharge), Pass 1 might process it in
  batches using safe-split points, but it's still all one encounter's entities

  Question to confirm: Does Pass 1 run BEFORE or AFTER Pass 0.5's reconciliation step? i.e.,
   does Pass 1 receive pending_encounter_id (pre-reconciliation) or healthcare_encounter_id
  (post-reconciliation)?

> important to note that whats important from pass05 onwards is not the pending encounter 
but rather the complete encounter that resides in the reconciled healthcare_encouners table.
 pending encounters are just mid-processing encounters pre-reconciliation process (when 
multiple 50 page chunks are reconciled and any cascading pending encoutners are joined 
together to form one encounter). There will be trace links that can be followed if we want 
to find out what pending encounter originated the clincial entity, or what 50page chunk 
gavve rise to the pending encounter that gave rise to the clincial entity, but none of that 
is important - as its all mid-processing data as per pass05 design. Then, if the complete 
post-reconciled encounter is very large, we will use the safe split points (that 
unfortunately i beleve currently only exist within the pass05-chunk-results table as an 
array inside a column for the pending encounter that that chunk gave rise to, but we can do 
what is needed to bring to light these safe-split point arrays, whether that be fancy code 
scripts that pull them out of their current location inside the chunk tables, or we edit 
pass05 so that the reconciliation process generates safe-split point arrays inside of the 
post-reconciled heatlhcare_encounters table for each complete healthcare encounter - I want 
you to explore and let me know which option you recommend.    And yes pass 1 definitly has 
to run after pass 05 is completely finsihed and after reconciliation, because we need the 
completed post-reconciled encounters. 


⏺ Got it. This is much clearer now.

  Corrected Understanding:

  Pass 0.5: OCR → Chunks → Pending Encounters → RECONCILIATION → healthcare_encounters 
  (final)
                                                                        ↓
  Pass 1:   Receives ONE healthcare_encounter (post-reconciled, complete)
                                                                        ↓
            Extracts entities for that encounter

  The Safe-Split Points Problem:

  Currently, safe-split points live in pass05_chunk_results tied to the pending encounter
  that chunk created. But after reconciliation, multiple pending encounters may merge into
  ONE healthcare_encounter. So the safe-split points are fragmented across the
  pre-reconciliation data.

  Two Options:

  Option A: Pull safe-splits at Pass 1 runtime (code solution)
  - When Pass 1 receives a healthcare_encounter_id, trace back through:
    - healthcare_encounters → pass05_reconciliation_log → pending_encounter_ids →
  pass05_chunk_results.safe_split_points
  - Aggregate all safe-split arrays from all contributing chunks
  - Merge/deduplicate and re-sort by Y-coordinate

  Pros: No schema changes, no Pass 0.5 changes
  Cons: Complex joins at runtime, safe-splits may have gaps/overlaps at chunk boundaries,
  harder to debug

  Option B: Reconciliation generates consolidated safe-splits (schema solution)
  - Add safe_split_points JSONB column to healthcare_encounters table
  - During reconciliation, merge all contributing chunks' safe-split arrays
  - Adjust Y-coordinates if needed (chunks may have different page offsets)
  - Store consolidated array directly on the final encounter

  Pros: Clean data model, Pass 1 just reads one field, easier to debug, safe-splits are
  "blessed" post-reconciliation
  Cons: Requires Pass 0.5 reconciliation code change + migration

  ---
  My Recommendation: Option B

  Reasons:
  1. Data locality - The safe-splits belong to the encounter, not the processing artifacts
  2. Single source of truth - No runtime joins or tracing
  3. Reconciliation is the right place - It's already merging encounter data, adding
  safe-split merging is natural
  4. Debugging - You can inspect healthcare_encounters.safe_split_points directly
  5. Future-proof - If reconciliation logic changes, safe-splits stay consistent

  The reconciliation step already knows which pending encounters are merging - it just needs
   to also merge their safe-split arrays at the same time.

  Question: Does this match your intuition? If yes, I'll factor this into the Pass 1 master
  design (noting that a Pass 0.5 enhancement is a prerequisite).


  > yes happy to go with option B - add it to the list of to do items in the file you are 
about to create for pass 1.  Also note that the trick to keep in mind for option B is that a
 chunk may have multiple pending encounters, but the array is on a page centric format, so 
when reconciling the arrays between chunks, we cna either first updat the pending encounters
 table to include the arrays of sage splits that are relevant to it based on the pages, and 
then the reconciliation process for the arrays will simply be adding them together for 
cascading pending encounters that get matched and reconciled into 1 complete encounter, or 
we could skip the pending encounter table completely adn go directly to getting the 
reconciler to add the reconciled array to the final encounter, byt focusing on the pages - 
deos that make sense, which option within option B do you recommend? 

⏺ Yes, makes sense. The safe-split array is page-centric (Y-coordinates per page), and a
  chunk can contain multiple pending encounters spanning different pages.

  Option B1: Route through pending_encounters first
  pass05_chunk_results.safe_split_points (page-centric array)
      ↓ (split by page ranges per pending encounter)
  pass05_pending_encounters.safe_split_points (subset relevant to that encounter's pages)
      ↓ (concatenate for cascading/merged encounters)
  healthcare_encounters.safe_split_points (final consolidated)

  Option B2: Skip pending_encounters, go direct to reconciler
  pass05_chunk_results.safe_split_points (page-centric array)
      ↓ (reconciler looks up page ranges from pending encounters it's merging)
      ↓ (pulls relevant safe-splits directly from chunk_results based on pages)
  healthcare_encounters.safe_split_points (final consolidated)

  ---
  My Recommendation: Option B2 (skip pending_encounters)

  Reasons:

  1. Pending encounters are transient - They exist only during processing. Adding a column
  there means extra writes that get thrown away after reconciliation.
  2. Reconciler already has all the context - It knows which pending encounters are merging,
   which chunks they came from, and which pages they span. It can pull safe-splits directly
  from chunk_results.
  3. Simpler data flow - One transformation step instead of two. Less chance of sync issues.
  4. Page-centric logic lives in one place - The reconciler does the page-range filtering
  once, rather than doing it twice (first when populating pending_encounters, then again
  when merging).
  5. No schema change to pending_encounters - Keeps that table lean for its actual purpose
  (mid-processing state).

  The reconciler just needs to:
  1. For each pending_encounter being merged, get its page range
  2. Look up the chunk_results that produced it
  3. Filter safe_split_points to only those pages
  4. Concatenate all filtered arrays
  5. Write to healthcare_encounters.safe_split_points

  Does that align with your thinking?

> Pending encounters are not transient (the data does nto get thrown away and will exist forever) but thell be used for anything other than auditing etc . Does that change your judgement? 

⏺ No, it doesn't change my recommendation. I still prefer Option B2.


> great, proceed with creating your reate: shared/docs/architecture/database-foundation-v3/ai-processing-v3/implementation-planning/pass-1-entity-detection/strategy-a/PASS1-STRATEGY-A-MASTER.md 
