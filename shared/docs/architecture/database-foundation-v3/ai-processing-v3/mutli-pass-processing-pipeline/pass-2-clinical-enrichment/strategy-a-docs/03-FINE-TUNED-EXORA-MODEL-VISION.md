# 02 - Fine-Tuned Exora AI Model Vision

**Created:** 2025-12-02
**Status:** Future Vision
**Owner:** Xavier Flanagan

---

## The Vision

Once Exora has processed ~100,000 medical records through the current pipeline, we will have high-quality training data to fine-tune a custom AI model that:

1. **Eliminates bridge schemas** - Model already "knows" output formats for each entity type
2. **Reduces medical code waterfall** - Model has internalized common code mappings
3. **Optimized for Australian healthcare** - Trained on actual AU document formats, terminology, pathology layouts

---

## Training Data Generated by Current Architecture

| Component | Training Signal |
|-----------|-----------------|
| Pass 2 | OCR text + context -> structured clinical entity |
| Pass 2.5 | Entity + context -> selected medical code |
| Bridge schemas | Document region type -> which schema applies |

The current "expensive" architecture is building the curriculum for the future model.

---

## Milestones

| Records Processed | Capability |
|-------------------|------------|
| 10,000 | Experiment with fine-tuning |
| 100,000 | Production-quality specialized model |
| 1,000,000+ | Potentially train from scratch |

---

## What Changes

| Current | Future (Post Fine-Tuning) |
|---------|---------------------------|
| Bridge schema prompts in every call | Model knows formats implicitly |
| 3-tier waterfall for every entity | Model predicts codes directly (with fallback to waterfall for novel codes) |
| Generic foundation model | Exora-specialized medical extraction model |

---

## Key Insight

The bridge schemas and waterfall system aren't throwaway work - they're generating the training data we'll need. Every processed document makes the future model better.

---

**Next Steps (When Ready):**
- Define training data export format
- Establish quality metrics for fine-tuned model evaluation
- Plan A/B testing framework (fine-tuned vs current pipeline)
